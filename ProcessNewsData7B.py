import unicodedata
import websocket
import json
import requests
import re
import os
import mysql.connector
import threading
import queue
import multiprocessing
from dotenv import load_dotenv
from openai import OpenAI  # OpenAI í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€
from langchain_text_splitters import RecursiveCharacterTextSplitter
from mysql.connector import pooling
from html import unescape

# ==========================================
# 0. í™˜ê²½ ë³€ìˆ˜ ë° ê¸°ë³¸ ì„¤ì •
# ==========================================

load_dotenv()

n_cpu_cores = multiprocessing.cpu_count()

# í•„ìˆ˜ ê°’ í™•ì¸
if not os.getenv("LS_ACCESS_TOKEN") or not os.getenv("DB_PASSWORD"):
    print("âŒ [Error] .env íŒŒì¼ì´ ì—†ê±°ë‚˜ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜(TOKEN, PASSWORD)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    exit(1)

# ==========================================
# 1. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ (Configuration)
# ==========================================

WS_URL = "wss://openapi.ls-sec.co.kr:9443/websocket"
API_BASE_URL = "https://openapi.ls-sec.co.kr:8080"
ACCESS_TOKEN = os.getenv("LS_ACCESS_TOKEN")

# ë¡œì»¬ Llama ì„œë²„ ì„¤ì •
LLM_SERVER_URL = "http://localhost:8080/v1"

DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "user": os.getenv("DB_USER", "admin"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME", "LLM")
}

news_queue = queue.Queue()

# DB Pool ìƒì„±
try:
    db_pool = pooling.MySQLConnectionPool(
        pool_name="news_pool",
        pool_size=10,
        pool_reset_session=True,
        **DB_CONFIG
    )
    print("âœ… [System] DB Connection Pool ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"âŒ [System] DB Pool ìƒì„± ì‹¤íŒ¨: {e}")
    exit(1)

# ==========================================
# 2. LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI í˜¸í™˜)
# ==========================================

print(f"â³ [System] ë¡œì»¬ LLM ì„œë²„({LLM_SERVER_URL})ì— ì—°ê²° ì„¤ì • ì¤‘...")

try:
    client = OpenAI(
        base_url=LLM_SERVER_URL,
        api_key="no-key-needed"  # ë¡œì»¬ ì„œë²„ë¼ í‚¤ ë¶ˆí•„ìš”
    )

    # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì„ íƒ ì‚¬í•­)
    # client.models.list()

    print("âœ… [System] LLM í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
except Exception as e:
    print(f"âš ï¸ [Warning] LLM ì„œë²„ ì—°ê²° ì„¤ì • ì¤‘ ì—ëŸ¬ (ì„œë²„ê°€ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”): {e}")

# LLM í˜¸ì¶œ ì§ë ¬í™”ë¥¼ ìœ„í•œ ì „ì—­ ë½ (ì„œë²„ ë¶€í•˜ ì¡°ì ˆìš©)
llm_lock = threading.Lock()

# í‚¤ì›Œë“œ ì •ì˜





from keywords import (
    MACRO_KEYWORDS,  # MACRO_KEYWORDS = ["ê¸ˆë¦¬", "í™˜ìœ¨", "CPI", "PPI", "FOMC", "ì—°ì¤€", "GDP", "ë¬¼ê°€", "ìœ ê°€"]
    GLOBAL_KEYWORDS, # GLOBAL_KEYWORDS = ["ë‚˜ìŠ¤ë‹¥", "ë‹¤ìš°", "S&P500", "ë‰´ìš•ì¦ì‹œ", "í…ŒìŠ¬ë¼", "ì—”ë¹„ë””ì•„", "ì• í”Œ", "TSMC"]
    DOMESTIC_MARKET_KEYWORDS, # DOMESTIC_MARKET_KEYWORDS = ["ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì™¸êµ­ì¸", "ê¸°ê´€", "ìˆœë§¤ìˆ˜", "ê³µë§¤ë„"]
    ETC_KEYWORDS, # ETC_KEYWORDS = ["ë¹„íŠ¸ì½”ì¸", "ê°€ìƒí™”í", "IPO", "ê³µëª¨ì£¼"]
)

# ==========================================
# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ì •ì œ, ë³‘í•©, DB)
# ==========================================

def clean_financial_text(text: str) -> str:
    """ê¸ˆìœµ í…ìŠ¤íŠ¸ ì •ì œ"""
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'^[A-Za-z0-9]+OutBlock\d+\s+', '', text)
    lines = text.splitlines()
    merged_lines = []

    bullet_pattern = re.compile(r'^[\*\-â€¢â€»\[]')
    finance_symbol_pattern = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]')
    starts_with_number = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]\s*[0-9\.]')

    for line in lines:
        line = line.strip()
        if not line:
            continue
        if not merged_lines:
            merged_lines.append(line)
            continue
        prev_line = merged_lines[-1]

        if bullet_pattern.match(line):
            merged_lines.append(line)
        elif finance_symbol_pattern.match(line):
            if starts_with_number.match(line):
                merged_lines[-1] += " " + line
            else:
                merged_lines.append(line)
        elif prev_line.endswith('.') or prev_line.endswith(':'):
            merged_lines.append(line)
        else:
            merged_lines[-1] += " " + line

    result = "\n".join(merged_lines)
    result = re.sub(r'(\n)([â–²â–¼â–³â–½â†‘â†“])(?!\s*[0-9])', r'\n\n\2', result)
    return result


def clean_base_text(text: str) -> str:
    """HTML ë° ë…¸ì´ì¦ˆ ì œê±°"""
    if not text or not isinstance(text, str):
        return ""
    text = unescape(text)
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'<.*?>', '', text, flags=re.DOTALL)
    text = re.sub(r'(@media.*?\{.*?\})|(\{.*?\})', '', text, flags=re.DOTALL)
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
    text = re.sub(r'^[A-Za-z0-9]+OutBlock\d+\s+', '', text)
    text = re.sub(r'[^\w\s.,\'"()%\+/\-â–²â–¼â–³â–½â†‘â†“]', '', text)
    return text


def merge_news_bodies(news_bodies):
    """ë‰´ìŠ¤ ë³¸ë¬¸ ë°°ì—´ ë³‘í•©"""
    merged_lines = []
    for news in news_bodies:
        line = news['sBody'].strip()
        if not line:
            continue
        if not merged_lines:
            merged_lines.append(line)
            continue
        if merged_lines[-1].endswith('.') or merged_lines[-1].endswith(':'):
            merged_lines.append(line)
        else:
            merged_lines[-1] += " " + line
    return "\n".join(merged_lines)


def insert_to_db(data):
    """MySQL ë°ì´í„° ì €ì¥"""
    conn = None
    try:
        conn = db_pool.get_connection()
        if conn.is_connected():
            cursor = conn.cursor()
            insert_query = """
                           INSERT INTO news_data (date, time, id, realkey, title, bodysize, category, body)
                           VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
                           """
            cursor.execute(insert_query, data)
            conn.commit()
    except mysql.connector.Error as err:
        print(f"âŒ DB ì—ëŸ¬: {err}")
    finally:
        if conn:
            conn.close()


# ==========================================
# 4. í‚¤ì›Œë“œ ê¸°ë°˜ 1ì°¨ ë¶„ë¥˜
# ==========================================

def quick_keyword_classify(title: str) -> str | None:
    t = title.strip()
    if any(k in t for k in MACRO_KEYWORDS):
        return "ê±°ì‹œê²½ì œ"
    if any(k in t for k in DOMESTIC_MARKET_KEYWORDS):
        return "êµ­ë‚´ ì‹œí™©"
    if any(k in t for k in GLOBAL_KEYWORDS):
        return "í•´ì™¸ ì¦ì‹œ"
    if any(k in t for k in ETC_KEYWORDS):
        return "ê¸°íƒ€"
    return None


# ==========================================
# 5. ìš”ì•½ + ë¶„ë¥˜ í†µí•© LLM í˜¸ì¶œ (OpenAI API ë°©ì‹)
# ==========================================

def call_llm_api(system_prompt, user_prompt, max_tokens=1024):
    """OpenAI ìŠ¤íƒ€ì¼ API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜"""
    try:
        # ìŠ¤ë ˆë“œ ë½ ì‚¬ìš© (ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€)
        with llm_lock:
            response = client.chat.completions.create(
                model="Qwen2.5-7B-Instruct-Q4_K_M.gguf",  # ë¡œì»¬ ì„œë²„ì—ì„œëŠ” ë¬´ì‹œë¨
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=max_tokens,
                stream=False
            )
            return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ [LLM API Error] {e}")
        return ""


def summarize_and_classify(text: str, title: str) -> tuple[str, str]:
    """
    1) ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ ë‹¨ìœ„ ìš”ì•½ í›„ ìµœì¢… ìš”ì•½
    2) ìµœì¢… ìš”ì•½ + ì œëª©ì„ ì´ìš©í•´ LLMìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    """
    category = "ê¸°íƒ€_LLM"

    if not text:
        return "ë‚´ìš© ì—†ìŒ", "ê¸°íƒ€"

    # 1ì°¨: í‚¤ì›Œë“œ ë¶„ë¥˜
    kw_category = quick_keyword_classify(title)

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
    chunks = text_splitter.split_text(text)

    summary_system_prompt = (
        "ë‹¹ì‹ ì€ ê³ ë„ë¡œ ìˆ™ë ¨ëœ ì „ë¬¸ ìš”ì•½ê°€ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìê°€ ì œê³µí•œ ë‰´ìŠ¤, ê¸°ì‚¬, ë˜ëŠ” ê¸°íƒ€ í…ìŠ¤íŠ¸ì—ì„œ íˆ¬ììê°€ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. "
        "ìš”ì•½ì€ ìµœëŒ€ 3~5ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤: "
        "1) ì¤‘ìš”í•œ ìˆ˜ì¹˜ë‚˜ í†µê³„, 2) ê´€ë ¨ ê¸°ì—…ëª… ë° ì¸ë¬¼, 3) ì£¼ìš” ì‚¬ê±´ ë° ì‹œì¥ ë™í–¥, 4) ê²½ì œì  ì˜í–¥ê³¼ ì „ë§. "
        "ë¶ˆí•„ìš”í•œ ë°°ê²½ ì„¤ëª…, ì¶”ì¸¡, ê°ì •ì  í‘œí˜„, ê³¼ë„í•œ ì„¸ë¶€ì‚¬í•­ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. "
        "ìš”ì•½ì€ ì œëª©ì´ë‚˜ í˜•ì‹ì ì¸ í‘œí˜„ ì—†ì´ ë°”ë¡œ ì‹œì‘í•˜ì„¸ìš”."
    )

    def _llm_summary_single(chunk_text: str) -> str:
        result = call_llm_api(summary_system_prompt, f"ë‰´ìŠ¤ ë³¸ë¬¸:\n{chunk_text}")
        # ë¶ˆí•„ìš”í•œ ì„œë‘ ì œê±°
        result = re.sub(r"^(\s*ìš”ì•½\s*[:\-\]]?|.*?ìš”ì•½í•´\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤[.]?)", "", result).strip()
        return result

    # 1) ì²­í¬ ìš”ì•½
    if len(chunks) == 1:
        print(f"ğŸ§© [System] í…ìŠ¤íŠ¸ 1ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        final_summary = _llm_summary_single(text)
    else:
        print(f"ğŸ§© [System] ê¸´ í…ìŠ¤íŠ¸ ë¶„í•  ì²˜ë¦¬ ({len(chunks)}ê°œ)")
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            print(f"   ... {i + 1}/{len(chunks)} ë²ˆì§¸ ì²­í¬ ìš”ì•½ ì¤‘")
            summary = _llm_summary_single(chunk)
            chunk_summaries.append(summary)

        combined = "\n".join(chunk_summaries)

        if len(combined) > 1000:
            print("ğŸ [System] ìµœì¢… ìš”ì•½ë³¸ ìƒì„± ì¤‘...")
            final_summary = _llm_summary_single(combined)
        else:
            print("ğŸ [System] ìµœì¢… ìš”ì•½ë³¸ ìƒì„± ...")
            final_summary = combined

    # 2) ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    if kw_category is not None:
        category = kw_category + "_KEY"
    else:
        classify_system = """
                ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤. ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ ë‰´ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.
                [ë¶„ë¥˜ ê¸°ì¤€]
                1. ê±°ì‹œê²½ì œ: ê¸ˆë¦¬, í™˜ìœ¨, ìœ ê°€, CPI, ì—°ì¤€(Fed), ê²½ì œì§€í‘œ.
                2. í•´ì™¸ ì¦ì‹œ: ë¯¸êµ­/í•´ì™¸ ì¦ì‹œ ì§€ìˆ˜, í•´ì™¸ ê¸°ì—….
                3. êµ­ë‚´ ì‹œí™©: ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì§€ìˆ˜, ì™¸êµ­ì¸/ê¸°ê´€ ìˆ˜ê¸‰.
                4. ì£¼ë„ ì„¹í„°: êµ­ë‚´ ê°œë³„ ê¸°ì—… ë° ì‚°ì—…(ë°˜ë„ì²´, 2ì°¨ì „ì§€, ë°”ì´ì˜¤).
                5. ê¸°íƒ€: ê°€ìƒìì‚°, ì •ì±…, IPO, ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ ë“±.

                ì˜¤ì§ ìœ„ 5ê°œ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
                """
        classify_user = (
            f"ë‰´ìŠ¤ ì œëª©: {title}\n\n"
            f"ë‰´ìŠ¤ ìš”ì•½:\n{final_summary}\n\n"
            f"ì´ ë‰´ìŠ¤ì˜ ì¹´í…Œê³ ë¦¬ëŠ”?"
        )

        result = call_llm_api(classify_system, classify_user, max_tokens=50)

        valid_categories = ["ê±°ì‹œê²½ì œ", "í•´ì™¸ ì¦ì‹œ", "êµ­ë‚´ ì‹œí™©", "ì£¼ë„ ì„¹í„°", "ê¸°íƒ€"]
        for cat in valid_categories:
            if cat in result:
                category = cat + "_LLM"
                break

    return final_summary, category


# ==========================================
# 6. ë‰´ìŠ¤ ë³¸ë¬¸ ì¡°íšŒ ë° êµ¬ì¡°í™”
# ==========================================

def refine_financial_structure(text: str) -> str:
    """ëŠì–´ì§„ ë¬¸ì¥ ì—°ê²° ë° ê¸ˆìœµ ê¸°í˜¸ êµ¬ì¡°í™”"""
    lines = text.splitlines()
    merged_lines = []
    bullet_pattern = re.compile(r'^[\*\-â€¢â€»\[]')
    finance_symbol_pattern = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]')
    starts_with_number = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]\s*[0-9\.]')

    for line in lines:
        line = line.strip()
        if not line: continue
        if not merged_lines:
            merged_lines.append(line)
            continue
        prev_line = merged_lines[-1]

        if bullet_pattern.match(line):
            merged_lines.append(line)
        elif finance_symbol_pattern.match(line):
            if starts_with_number.match(line):
                merged_lines[-1] += " " + line
            else:
                merged_lines.append(line)
        elif prev_line.endswith('.') or prev_line.endswith(':'):
            merged_lines.append(line)
        else:
            merged_lines[-1] += " " + line

    result = "\n".join(merged_lines)
    result = re.sub(r'(\n)([â–²â–¼â–³â–½â†‘â†“])(?!\s*[0-9])', r'\n\n\2', result)
    result = re.sub(r'[ \t]+', ' ', result)
    return result


def get_headers(tr_cd, tr_cont="N"):
    return {
        "Content-Type": "application/json; charset=UTF-8",
        "Authorization": f"Bearer {ACCESS_TOKEN}",
        "tr_cd": tr_cd,
        "tr_cont": tr_cont,
        "mac_address": "00:11:22:33:44:55"
    }


def fetch_news_body(news_id):
    """REST APIë¥¼ í†µí•´ ë‰´ìŠ¤ ìƒì„¸ ë³¸ë¬¸ ì¡°íšŒ"""
    url = f"{API_BASE_URL}/stock/investinfo"
    headers = get_headers("t3102")
    data = {"t3102InBlock": {"sNewsno": news_id}}

    try:
        response = requests.post(url, headers=headers, data=json.dumps(data), verify=False)
        if response.status_code == 200:
            print("ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì²­ ì„±ê³µ!")
            response_json = response.json()
            if "t3102OutBlock1" not in response_json:
                print("ë‰´ìŠ¤ ë³¸ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
                return None

            news_body = response_json["t3102OutBlock1"]
            joined_body = merge_news_bodies(news_body)
            cleaned_body = clean_base_text(joined_body)
            refined_body = refine_financial_structure(cleaned_body)
            return refined_body
        else:
            print(f"ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ ì¡°íšŒ ì‹¤íŒ¨ ({news_id}): {e}")
    return None


# ==========================================
# 7. ì›Œì»¤ ìŠ¤ë ˆë“œ
# ==========================================

def worker():
    """ëŒ€ê¸°ì—´ì—ì„œ ë‰´ìŠ¤ë¥¼ êº¼ë‚´ ì²˜ë¦¬í•˜ëŠ” ì†Œë¹„ì í•¨ìˆ˜"""
    print("ğŸš€ ë‰´ìŠ¤ ì²˜ë¦¬ ì›Œì»¤(Worker) ì‹œì‘ë¨...")
    while True:
        news_item = news_queue.get()
        try:
            if news_item is None:
                print("ğŸ›‘ ì›Œì»¤ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                return

            # debug (ì‚¬ìš©ì ìš”ì²­: ê¸°ì¡´ print ìœ ì§€)
            print(f"ë‚ ì§œ: {news_item.get('date')}")
            print(f"ì‹œê°„: {news_item.get('time')}")
            print(f"í‚¤ê°’: {news_item.get('realkey')}")
            print(f"ì œëª©: {news_item.get('title')}")

            raw_date = news_item.get('date')
            raw_time = news_item.get('time')
            raw_title = news_item.get('title')
            news_id = news_item.get('id')
            realkey = news_item.get('realkey')

            title = clean_financial_text(raw_title)
            date = f"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:]}" if raw_date and len(raw_date) == 8 else raw_date
            time_str = f"{raw_time[:2]}:{raw_time[2:4]}:{raw_time[4:]}" if raw_time and len(raw_time) == 6 else raw_time

            print(f"\nğŸ”„ ì²˜ë¦¬ ì‹œì‘: {title}")

            # 1. ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            raw_body = fetch_news_body(realkey)

            if raw_body:
                print("\në‰´ìŠ¤ ë³¸ë¬¸:")
                print(raw_body + "...\n")  # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ì•ë¶€ë¶„ë§Œ ì¶œë ¥

                # 2. ìš”ì•½ + ë¶„ë¥˜ (OpenAI API ì‚¬ìš©)
                summary_body, category = summarize_and_classify(raw_body, title)

                db_data = (
                    date,
                    time_str,
                    news_id,
                    realkey,
                    title,
                    len(raw_body),
                    category,
                    summary_body
               )

                # 3. DB ì €ì¥
                if "ê¸°íƒ€" not in category:    # and "ì£¼ë„ ì„¹í„°" not in category
                    insert_to_db(db_data)
                    print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {title} (ì¹´í…Œê³ ë¦¬: {category})")
                else:
                    print(f"ğŸš« '{category}' ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ë˜ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {title}\n")
            else:
                print("âš ï¸ ë³¸ë¬¸ ì—†ìŒ, ê±´ë„ˆëœ€.")

        except Exception as e:
            print(f"\nâŒ ì›Œì»¤ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        finally:
            news_queue.task_done()


# ==========================================
# 8. WebSocket ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
# ==========================================

def on_message(ws, message):
    try:
        response = json.loads(message)
        if "body" in response:
            news_data = response["body"]
            news_queue.put(news_data)
            print(f"ğŸ“© [ìˆ˜ì‹ ] {news_data.get('title')} -> ëŒ€ê¸°ì—´ ì¶”ê°€ë¨")
    except Exception as e:
        print(f"ë©”ì‹œì§€ íŒŒì‹± ì—ëŸ¬: {e}")


def on_open(ws):
    print("ğŸŒ WebSocket ì—°ê²° ë° êµ¬ë… ìš”ì²­")
    sub_msg = {
        "header": {"token": ACCESS_TOKEN, "tr_type": "3"},
        "body": {"tr_cd": "NWS", "tr_key": "NWS001"}
    }
    ws.send(json.dumps(sub_msg))


# ==========================================
# 9. ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================

if __name__ == "__main__":
    # ì›Œì»¤ ìˆ˜ ì„¤ì •
    num_workers = min(3, max(1, n_cpu_cores - 2))
    print(f"ğŸ§µ ì›Œì»¤ ìŠ¤ë ˆë“œ ìˆ˜: {num_workers}")

    worker_threads = []
    for _ in range(num_workers):
        t = threading.Thread(target=worker, daemon=True)
        t.start()
        worker_threads.append(t)

    websocket.enableTrace(False)
    ws_app = websocket.WebSocketApp(
        WS_URL,
        on_message=on_message,
        on_open=on_open,
        on_close=lambda ws, status_cd, msg: print("WebSocket ì—°ê²° ì¢…ë£Œ:", status_cd, msg),
        on_error=lambda ws, error: print("WebSocket ì—ëŸ¬:", error)
    )

    try:
        ws_app.run_forever()
    except KeyboardInterrupt:
        print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì¤‘...")
        for _ in worker_threads:
            news_queue.put(None)
        for t in worker_threads:
            t.join()
